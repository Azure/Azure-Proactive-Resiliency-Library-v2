- description: Databricks runtime version is not latest or is not LTS version
  aprlGuid: 0e835cc2-2551-a247-b1f1-3c5f25c9cb70
  recommendationTypeId: null
  recommendationControl: Governance
  recommendationImpact: Medium
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Databricks recommends migrating workloads to the latest or LTS version of its runtime for enhanced stability and support. If on Runtime 11.3 LTS or above, move directly to the latest 12.x version. If below, first migrate to 11.3 LTS, then to the latest 12.x version as per the migration guide.
  potentialBenefits: Enhanced stability and support
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Databricks runtime support lifecycles
      url: "https://learn.microsoft.com/en-us/azure/databricks/release-notes/runtime/databricks-runtime-ver"

- description: Use Databricks Pools
  aprlGuid: c166602e-0804-e34b-be8f-09b4d56e1fcd
  recommendationTypeId: null
  recommendationControl: Scalability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Databricks pools pre-provision VMs, reducing risks of provisioning errors during cluster start or scale, enhancing reliability.
  potentialBenefits: Reduces provisioning errors
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Use SSD backed VMs for Worker VM Type and Driver type
  aprlGuid: 5877a510-8444-7a4c-8412-a8dab8662f7e
  recommendationTypeId: null
  recommendationControl: Scalability
  recommendationImpact: Medium
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Upgrade HDDs in premium VMs to SSDs for better speed and reliability. Premium SSDs boost IO-heavy apps; Standard SSDs balance cost and performance. Ideal for critical workloads, upgrading improves connectivity with brief reboot. Consider for vital VMs
  potentialBenefits: Faster, reliable VM performance
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Azure managed disk types
      url: "https://learn.microsoft.com/azure/virtual-machines/disks-types#premium-ssd"

- description: Enable autoscaling for batch workloads
  aprlGuid: 5c72f0d6-55ec-d941-be84-36c194fa78c0
  recommendationTypeId: null
  recommendationControl: Scalability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Autoscaling adjusts cluster sizes automatically based on workload demands, offering benefits for many use cases in terms of costs and performance. It includes guidance on when and how to best utilize Autoscaling. For streaming, Delta Live Tables with autoscaling is advised.
  potentialBenefits: Cost and performance optimization
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices#enable-autoscaling-for-batch-workloadss"

- description: Enable autoscaling for SQL warehouse
  aprlGuid: 362ad2b6-b92c-414f-980a-0cf69467ccce
  recommendationTypeId: null
  recommendationControl: Scalability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    The scaling parameter of a SQL warehouse defines the min and max number of clusters for distributing queries. By default, it's set to one. Increasing the cluster count can accommodate more concurrent users effectively.
  potentialBenefits: Improves concurrency and efficiency
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices#enable-autoscaling-for-sql-warehouse"

- description: Use Delta Live Tables enhanced autoscaling
  aprlGuid: cd77db98-9b13-6e4b-bd2b-74c2cb538628
  recommendationTypeId: null
  recommendationControl: Scalability
  recommendationImpact: Medium
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Databricks enhanced autoscaling optimizes cluster utilization by automatically allocating cluster resources based on workload volume, with minimal impact on the data processing latency of your pipelines.
  potentialBenefits: Optimized resource use and minimal latency
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/azure/databricks/lakehouse-architecture/reliability/best-practices"
    - name: Databricks enhanced autoscaling
      url: "https://learn.microsoft.com/azure/databricks/delta-live-tables/settings#use-autoscaling-to-increase-efficiency-and-reduce-resource-usage"

- description: Automatic Job Termination is enabled, ensure there are no user-defined local processes
  aprlGuid: 3d3e53b5-ebd1-db42-b43b-d4fad74824ec
  recommendationTypeId: null
  recommendationControl: High Availability
  recommendationImpact: Medium
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    To conserve cluster resources, you can terminate a cluster to store its configuration for future reuse or autostart jobs. Clusters can auto-terminate after inactivity, but this only tracks Spark jobs, not local processes, which might still be running even after Spark jobs end.
  potentialBenefits: Saves cluster resources, avoids idle use
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability?
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Enable Logging-Cluster log delivery
  aprlGuid: 7fb90127-5364-bb4d-86fa-30778ed713fb
  recommendationTypeId: null
  recommendationControl: Monitoring and Alerting
  recommendationImpact: Medium
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    When creating a Databricks cluster, you can set a log delivery location for the Spark driver, worker nodes, and events. Logs are delivered every 5 mins and archived hourly. Upon cluster termination, all generated logs until that point are guaranteed to be delivered.
  potentialBenefits: Improved troubleshooting and audit
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Create a cluster
      url: "https://learn.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-log-delivery"

- description: Use Delta Lake for higher reliability
  aprlGuid: da4ea916-4df3-8c4d-8060-17b49da45977
  recommendationTypeId: null
  recommendationControl: High Availability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Delta Lake is an open source storage format enhancing data lakes' reliability with ACID transactions, schema enforcement, and scalable metadata handling.
  potentialBenefits: Enhances data reliability and processing
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Use Photon Acceleration
  aprlGuid: 892ca809-e2b5-9a47-924a-71132bf6f902
  recommendationTypeId: null
  recommendationControl: High Availability
  recommendationImpact: Low
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Apache Spark in Databricks Lakehouse ensures resilient distributed data processing by automatically rescheduling failed tasks, aiding in overcoming external issues like network problems or revoked VMs.
  potentialBenefits: Boosts speed and reliability for Spark tasks
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices#use-apache-spark-or-photon-for-distributed-compute"

- description: Automatically rescue invalid or nonconforming data with Databricks Auto Loader or Delta Live Tables
  aprlGuid: 7e52d64d-8cc0-8548-a593-eb49ab45630d
  recommendationTypeId: null
  recommendationControl: Business Continuity
  recommendationImpact: Low
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Invalid or nonconforming data can crash workloads dependent on specific data formats. Best practices recommend filtering such data at ingestion to improve end-to-end resilience, ensuring no data is lost or missed.
  potentialBenefits: Enhanced data resilience and integrity
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Configure jobs for automatic retries and termination
  aprlGuid: 84e44da6-8cd7-b349-b02c-c8bf72cf587c
  recommendationTypeId: null
  recommendationControl: High Availability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Use Databricks and MLflow for deploying models as Spark UDFs for job scheduling, retries, autoscaling. Model serving offers scalable infrastructure, processes models using MLflow, and serves them via REST API using serverless compute managed in Databricks cloud.
  potentialBenefits: Enhanced reliability and autoscaling
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Use a scalable and production-grade model serving infrastructure
  aprlGuid: 4cbb7744-ff3d-0447-badb-baf068c95696
  recommendationTypeId: null
  recommendationControl: Scalability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Use Databricks and MLflow for deploying models as Apache Spark UDFs, benefiting from job scheduling, retries, autoscaling, etc.
  potentialBenefits: Enhances scalability and reliability
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Use a layered storage architecture
  aprlGuid: 1b0d0893-bf0e-8f4c-9dc6-f18f145c1ecf
  recommendationTypeId: null
  recommendationControl: High Availability
  recommendationImpact: Medium
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Curate data by creating a layered architecture to increase data quality across layers. Start with a raw layer for ingested source data, continue with a curated layer for cleansed and refined data, and finish with a final layer catered to business needs, focusing on security and performance.
  potentialBenefits: Enhances data quality and trust
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Improve data integrity by reducing data redundancy
  aprlGuid: e93fe702-e385-d741-ba37-1f1656482ecd
  recommendationTypeId: null
  recommendationControl: Business Continuity
  recommendationImpact: Low
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Copying data leads to redundancy, lost integrity, lineage, and access issues, affecting lakehouse data quality. Temporary copies are useful for agility and innovation but can become problematic operational data silos, questioning data's master status and currency.
  potentialBenefits: Enhanced data integrity and quality
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Actively manage schemas
  aprlGuid: b7e1d13f-54c9-1648-8a52-34c0abe8ce16
  recommendationTypeId: null
  recommendationControl: Other Best Practices
  recommendationImpact: Medium
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Uncontrolled schema changes can lead to invalid data and failing jobs. Databricks validates and enforces schema through Delta Lake, which prevents bad records during ingestion, and Auto Loader, which detects new columns and supports schema evolution to maintain data integrity.
  potentialBenefits: Prevents invalid data and job failures
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Use constraints and data expectations
  aprlGuid: a42297c4-7e4f-8b41-8d4b-114033263f0e
  recommendationTypeId: null
  recommendationControl: Business Continuity
  recommendationImpact: Low
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Delta tables verify data quality automatically with SQL constraints, triggering an error for violations. Delta Live Tables enhance this by defining expectations for data quality, utilizing Python or SQL, to manage actions for record failures, ensuring data integrity and compliance.
  potentialBenefits: Ensures data quality and integrity
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices#use-constraints-and-data-expectations"

- description: Create regular backups
  aprlGuid: 932d45d6-b46d-e341-abfb-d97bce832f1f
  recommendationTypeId: null
  recommendationControl: Disaster Recovery
  recommendationImpact: Low
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    To recover from a failure, regular backups are needed. The Databricks Labs project migrate lets admins create backups by exporting workspace assets using the Databricks CLI/API. These backups help in restoring or migrating workspaces.
  potentialBenefits: Ensures data recovery and migration
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices#create-regular-backups"

- description: Recover from Structured Streaming query failures
  aprlGuid: 12e9d852-5cdc-2743-bffe-ee21f2ef7781
  recommendationTypeId: null
  recommendationControl: High Availability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Structured Streaming ensures fault-tolerance and data consistency in streaming queries. With Azure Databricks workflows, you can set up your queries to automatically restart after failure, picking up precisely where they left off.
  potentialBenefits: Fault-tolerance and auto-restart for queries
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices#recover-from-structured-streaming-query-failures"

- description: Recover ETL jobs based on Delta time travel
  aprlGuid: a18d60f8-c98c-ba4e-ad6e-2fac72879df1
  recommendationTypeId: null
  recommendationControl: Disaster Recovery
  recommendationImpact: Medium
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Despite thorough testing, a production job can fail or yield unexpected data. Sometimes, repairs are done by adding jobs post-issue identification and pipeline correction.
  potentialBenefits: Easy rollback and fix for ETL jobs
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices#recover-etl-jobs-based-on-delta-time-travel"

- description: Use Databricks Workflows and built-in recovery
  aprlGuid: c0e22580-3819-444d-8546-a80e4ed85c83
  recommendationTypeId: null
  recommendationControl: Disaster Recovery
  recommendationImpact: Low
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Databricks Workflows enable efficient error recovery in multi-task jobs by offering a matrix view for issue examination. Fixes can be applied to initiate repair runs targeting only failed and dependent tasks, preserving successful outcomes and thereby saving time and money.
  potentialBenefits: Saves time and money with smart recovery
  pgVerified: true
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for reliability
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/reliability/best-practices"

- description: Configure a disaster recovery pattern
  aprlGuid: 4fdb7112-4531-6f48-b60e-c917a6068d9b
  recommendationTypeId: null
  recommendationControl: Disaster Recovery
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Implementing a disaster recovery pattern is vital for Azure Databricks, a cloud-native data analytics platform, ensuring data teams' access even during rare regional outages caused by disasters like hurricanes or earthquakes.
  potentialBenefits: Ensures service continuity during disasters
  pgVerified: false
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Azure Databricks Best Practices
      url: "https://github.com/Azure/AzureDatabricksBestPractices/tree/master"

- description: Automate deployments and workloads
  aprlGuid: 42aedaa8-6151-424d-b782-b8666c779969
  recommendationTypeId: null
  recommendationControl: Other Best Practices
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    The Databricks Terraform provider manages Azure Databricks workspaces and cloud infrastructure flexibly and powerfully.
  potentialBenefits: Efficient, reliable automation
  pgVerified: false
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for operational excellence
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/operational-excellence/best-practices#2-automate-deployments-and-workloads"

- description: Set up monitoring, alerting, and logging
  aprlGuid: 20193ff9-dbcd-a74e-b197-71d7d9d3c1e6
  recommendationTypeId: null
  recommendationControl: Monitoring and Alerting
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    The Databricks Terraform provider is a flexible, powerful tool for managing Azure Databricks workspaces and cloud infrastructure.
  potentialBenefits: Enhanced reliability and automation
  pgVerified: false
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Best practices for operational excellence
      url: "https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/operational-excellence/best-practices#system-monitoring"

- description: Deploy workspaces in separate Subscriptions
  aprlGuid: 397cdebb-9d6e-ab4f-83a1-8c481de0a3a7
  recommendationTypeId: null
  recommendationControl: Scalability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Customers often naturally divide workspaces by teams or departments. However, it's crucial to also consider Azure Subscription and ADB Workspace limits when partitioning.
  potentialBenefits: Enhanced limits management, team separation
  pgVerified: false
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Azure Databricks Best Practices
      url: "https://github.com/Azure/AzureDatabricksBestPractices/blob/master/toc.md#deploy-workspaces-in-multiple-subscriptions-to-honor-azure-capacity-limits"

- description: Isolate each workspace in its own Vnet
  aprlGuid: 5e722c4f-415a-9b4c-bd4c-96b74dce29ad
  recommendationTypeId: null
  recommendationControl: Scalability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Deploying only one Databricks Workspace per VNet aligns with ADB's isolation model.
  potentialBenefits: Enhanced security and resource isolation
  pgVerified: false
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Azure Databricks Best Practices
      url: "https://github.com/Azure/AzureDatabricksBestPractices/blob/master/toc.md#consider-isolating-each-workspace-in-its-own-vnet"

- description: Do not Store any Production Data in Default DBFS Folders
  aprlGuid: 14310ba6-77ad-3641-a2db-57a2218b9bc7
  recommendationTypeId: null
  recommendationControl: High Availability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Driven by security and data availability concerns, each Azure Databricks Workspace comes with a default DBFS designed for system-level artifacts like libraries and Init scripts, not for production data.
  potentialBenefits: Enhanced security, data protection
  pgVerified: false
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Azure Databricks Best Practices
      url: "https://github.com/Azure/AzureDatabricksBestPractices/blob/master/toc.md#do-not-store-any-production-data-in-default-dbfs-foldersr"

- description: Do not use Azure Spot VMs for critical Production workloads
  aprlGuid: b5af7e26-3939-1b48-8fba-f8d4a475c67a
  recommendationTypeId: null
  recommendationControl: High Availability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Azure Spot VMs are not suitable for critical production workloads needing high availability and reliability. They are meant for fault-tolerant tasks and can be evicted with 30-seconds notice if Azure needs the capacity, with no SLA guarantees.
  potentialBenefits: Ensures high reliability for production
  pgVerified: false
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Use Azure Spot Virtual Machines
      url: "https://learn.microsoft.com/en-us/azure/virtual-machines/spot-vms"

- description: Evaluate regional isolation for workspaces
  aprlGuid: 8aa63c34-dd9d-49bd-9582-21ec310dfbdd
  recommendationTypeId: null
  recommendationControl: High Availability
  recommendationImpact: High
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Move workspaces to in-region control plane for increased regional isolation. Identify current control plane region using the workspace URL and nslookup. When region from CNAME differs from workspace region and an in-region control is available, consider migration using tools provided below.
  potentialBenefits: Improves resilience and data sovereignty
  pgVerified: false
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Azure Databricks control plane addresses
      url: "https://learn.microsoft.com/azure/databricks/resources/supported-regions#--azure-databricks-control-plane-addresses"
    - name: Migrate - maintained by Databricks Inc.
      url: "https://github.com/databrickslabs/migrate"
    - name: Databricks Terraform Exporter - maintained by Databricks Inc. (Experimental)
      url: "https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/experimental-exporter"

- description: Define alternate VM SKUs
  aprlGuid: 028593be-956e-4736-bccf-074cb10b92f4
  recommendationTypeId: null
  recommendationControl: Personalized
  recommendationImpact: Medium
  recommendationResourceType: Microsoft.Databricks/workspaces
  recommendationMetadataState: Active
  longDescription: |
    Azure Databricks planning should include VM SKU swap strategies for capacity issues. VMs are regional, and allocation failures may occur, shown by a "CLOUD PROVIDER" error.
  potentialBenefits: Ensures service availability
  pgVerified: false
  publishedToLearn: false
  publishedToAdvisor: false
  automationAvailable: no
  tags: null
  learnMoreLink:
    - name: Compute configuration best practices
      url: "https://learn.microsoft.com/azure/databricks/compute/cluster-config-best-practices"
    - name: GPU-enabled compute
      url: "https://learn.microsoft.com/azure/databricks/compute/gpu"
